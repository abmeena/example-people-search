#!/usr/bin/env ruby

require 'faraday_middleware'
require 'typhoeus/adapters/faraday'
require 'multi_json'
require 'ruby-progressbar'
require 'terminal-table'
require 'logger'
require 'csv'

require 'dotenv'
Dotenv.load

logger       = Logger.new(File.expand_path('../../log/development.log', __FILE__))
requests_log = Logger.new(File.expand_path('../../log/requests.log', __FILE__))

connection = Faraday.new(url: ENV.fetch('API_ENDPOINT')) do |c|
  c.response(:logger, requests_log)
  c.adapter(:typhoeus)
end

connection.headers['Content-Type'] = 'application/json'

query_params = {
  :api_key => ENV.fetch('API_KEY'),
  :report => {
    :start_date => ENV.fetch('REPORT_START_DATE'),
    :end_date   => ENV.fetch('REPORT_END_DATE')
  }
}

report_url = "/v1/reports/%s/run" % [ENV.fetch('REPORT_GUID')]

logger.info("Starting the People Search Report query to %s" % [report_url])

request = connection.post(report_url, MultiJson.dump(query_params))

case(request.status)
when(202)
  status_url = request.headers.fetch('location')

  query_params = {
    :api_key => ENV.fetch('API_KEY'),
    :limit   => ENV.fetch('RESULTS_LIMIT', 100)
  }

  status_request = connection.get(status_url, query_params)

  case(status_request.status)
  when 201
  when 200, 201
    logger.info("Polling the status from %s" % [status_url])
    progress_bar = ProgressBar.create(title: 'Results status', starting_at: 0, total: 1.0)

    # Poll the status until it's completed
    next_request = loop do
      poll_status_request  = connection.get(status_url, query_params)
      poll_status_response = MultiJson.load(poll_status_request.body)

      break poll_status_request if poll_status_response['completed'] == true || (poll_status_response.has_key?('error') && !poll_status_response['error'].nil?)
      progress_bar.progress = poll_status_response['progress']
      logger.info("Progress: %s" % [poll_status_response['progress']])
    end
    progress_bar.finish

    case(next_request.status)
    when 201
      results_url = next_request.headers.fetch('location')

      logger.info("Retrieving results from %s" % [results_url])

      results_request = connection.get(results_url, query_params)

      case(results_request.status)
      when 200
        rows = []

        results_response = MultiJson.load(results_request.body)

        rows.concat(results_response.fetch('rows', []))

        links     = results_response.fetch('links', [])
        next_link = links.select { |r| r['rel'] == 'next' }.first

        if next_link
          loop do
            logger.info("Retrieving paginated set %s" % [next_link['href']])

            results_request  = connection.get(next_link['href'])
            results_response = MultiJson.load(results_request.body)

            links     = results_response.fetch('links', [])
            next_link = links.select { |r| r['rel'] == 'next' }.first

            rows.concat(results_response.fetch('rows', []))
            break if next_link.nil?
          end
        end

        header_display = ->(row) do
          display_return_option = row.fetch('display_return_option', nil)

          val = ''
          val << row.fetch('display_name', nil)

          if display_return_option
            val << " (%s)" % [display_return_option]
          end

          val
        end

        columns  = results_response.fetch('columns', [])
        headings = columns.map { |r| header_display.call(r) }

        # Output to the console
        table = Terminal::Table.new(title: 'Query Results', headings: headings, rows: rows)
        puts table.to_s

        # Write to a final JSON file
        final_response_body = {
          :total    => results_response.fetch('total', 0),
          :metadata => results_response.fetch('metadata', {}),
          :columns  => columns,
          :rows     => rows
        }

        logger.info('Writing to output/results.json cache')
        File.open(File.expand_path('../../cache/results.json', __FILE__), 'w+') do |f|
          f.write(MultiJson.dump(final_response_body))
        end

        logger.info('Writing to output/results.csv cache')
        CSV.open(File.expand_path('../../cache/results.csv', __FILE__), 'w+') do |csv|
          csv << headings
          rows.each { |r| csv << r }
        end
      else
        logger.error("[Results Request] There was an error (%s)\n%s" % [results_request.status, results_request.inspect])
      end
    else
      logger.error("[Status Ping Request] There was an error (%s)\n%s" % [next_request.status, next_request.inspect])
    end
  else
    logger.error("[Initial Status Request] There was an error (%s)\n%s" % [status_request.status, status_request.inspect])
  end
else
  logger.error("[Initial Request] There was an error (%s)\n%s" % [request.status, request.inspect])
end

